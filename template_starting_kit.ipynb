{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <a href=\"https://www.dataia.eu/\">\n",
    "        <img border=\"0\" src=\"https://github.com/ramp-kits/template-kit/raw/main/img/DATAIA-h.png\" width=\"90%\"></a>\n",
    "</div>\n",
    "\n",
    "# Template Kit for RAMP challenge\n",
    "\n",
    "<i> Thomas Moreau (Inria) </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<p style=\"text-align: justify;\">\n",
    "    At the start of the year, the Court of Auditors published its report on the evaluation of the Skills Investment Plan, implemented during Emmanuel Macron’s first term from 2017 to 2022. This plan allocated 15 billion euros over five years to tackle youth and long-term unemployment. However, the report highlights an uneven distribution of funds across regions and years, with some areas receiving surplus funding while others faced shortages. <br>\n",
    "</p>\n",
    "<p style=\"text-align: justify;\">\n",
    "    Our project proposes <b>a solution for reallocating resources annually based on predicted funding needs at the departmental level. </b> Indeed, at the end of each year N-1, we aim to forecast the number of young job seekers for year N. This prediction will help the State better assess the potential regional needs for training funding, as it must depend on the number of unemployed individuals. Thus this result would help optimize the allocation of financial resources between regions and improve the efficiency of future public policies. <br>\n",
    "</p>\n",
    "<p style=\"text-align: justify;\">\n",
    "    For this project, we chose to focus on young job seekers at the departmental level. Our target variable will therefore be: <b>\"the number of job seekers under 25yo in department D for year N\".</b> To build this prediction, we rely on the Workforce Needs Survey conducted by France Travail in the last quarter of each year. This survey collects data from 2 million private-sector companies, asking about their expected job creations for the coming year and the challenges they anticipate in filling these positions (such as skill shortages or job difficulty). In addition, we will integrate other key indicators from year N-1, including the number of job postings, completed training programs, and control variables like the department’s population.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "This dataset is designed to predict youth unemployment (ages 15-24) in France using indicators from the French labor market. The data comes from *France Travail*, specifically from *the Statistiques et Analyses* section, covering the period from 2015 to 2023.\n",
    "### The dataset includes the following key features:\n",
    "\n",
    "- Year\n",
    "- Department\n",
    "- Number of workforce needs declared by companies, indicating recruitment demand across different sectors for this year and department.\n",
    "- Recruitment difficulty index (0-100%), showing the percentage of difficulty companies face when hiring for this year and by department.\n",
    "- Number of unemployed youth (15-24 years old) in France for the **previous** year and by department.\n",
    "- Number of training programs offered for job seekers for the previous year and by department, providing insights into workforce skill development.\n",
    "- Number of job offers available for the previous year and by department, reflecting labor market demand.\n",
    "- Number of people entering and exiting the unemployment lists in France for the previous year and by department, providing a dynamic view of job market inflows and outflows.\n",
    "- The size of the population for the previous year and by department (*Insee*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "The goal of this section is to show what's in the data, and how to play with it.\n",
    "This is the first set in any data science project, and here, you should give a sense of the data the participants will be working with.\n",
    "\n",
    "You can first load and describe the data, and then show some interesting properties of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the data\n",
    "import problem\n",
    "X_df, y = problem.get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11',\n",
       "       '12', '13', '14', '15', '16', '17', '18', '19', '21', '22', '23',\n",
       "       '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34',\n",
       "       '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45',\n",
       "       '46', '47', '49', '50', '51', '52', '53', '54', '55', '56', '57',\n",
       "       '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68',\n",
       "       '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79',\n",
       "       '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90',\n",
       "       '91', '92', '93', '94', '95'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.GEO.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7440.833333\n",
       "1    11160.833333\n",
       "2     5078.333333\n",
       "3     2411.666667\n",
       "4     1920.833333\n",
       "Name: OBS_VALUE, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(X_df.TIME_PERIOD.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge evaluation\n",
    "\n",
    "A particularly important point in a challenge is to describe how it is evaluated. This is the section where you should describe the metric that will be used to evaluate the participants' submissions, as well as your evaluation strategy, in particular if there is some complexity in the way the data should be split to ensure valid results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission format\n",
    "\n",
    "Here, you should describe the submission format. This is the format the participants should follow to submit their predictions on the RAMP plateform.\n",
    "\n",
    "This section also show how to use the `ramp-workflow` library to test the submission locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline workflow\n",
    "\n",
    "The input data are stored in a dataframe. To go from a dataframe to a numpy array we will use a scikit-learn column transformer. The first example we will write will just consist in selecting a subset of columns we want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['TIME_PERIOD', 'GEO', 'number_courses', 'job_offer',\n",
       "       'need_for_manpower', 'difficult_recruitment', 'out_of_list',\n",
       "       'entry_on_list', 'population'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load submissions/starting_kit/estimator.py\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cols = ['TIME_PERIOD', 'GEO', 'number_courses', 'job_offer',\n",
    "       'need_for_manpower', 'difficult_recruitment', 'out_of_list',\n",
    "       'entry_on_list', 'population']\n",
    "\n",
    "categorical_cols = ['GEO', 'TIME_PERIOD']\n",
    "numerical_cols = ['number_courses', 'job_offer',\n",
    "       'need_for_manpower', 'difficult_recruitment', 'out_of_list',\n",
    "       'entry_on_list', 'population']\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ('passthrough', numerical_cols)\n",
    ")\n",
    "\n",
    "def get_estimator():\n",
    "    pipe = make_pipeline(\n",
    "        transformer,\n",
    "        SimpleImputer(strategy='most_frequent'),\n",
    "        LinearRegression()\n",
    "    )\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[362.25196214 261.46333993 207.06763228 254.56361181 498.42648144]\n"
     ]
    }
   ],
   "source": [
    "from skrub import tabular_learner\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tabular_learner('regressor'), X_df, y, scoring='neg_median_absolute_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using a scikit-learn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 834.98732666  984.5303425  2197.00422415]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(get_estimator(), X_df, y, cv=3, scoring='neg_median_absolute_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 629.32549198  814.16355456 1181.18506594]\n"
     ]
    }
   ],
   "source": [
    "X_df_test, y_test = problem.get_test_data()\n",
    "scores = cross_val_score(get_estimator(), X_df_test, y_test, cv=3, scoring='neg_median_absolute_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "To submit your code, you can refer to the [online documentation](https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
